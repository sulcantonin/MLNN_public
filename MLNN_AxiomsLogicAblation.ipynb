{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Axiom $T$"
      ],
      "metadata": {
        "id": "5Ol7oJAjGYqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C8oUBfZ86cU",
        "outputId": "610008a3-8fba-4dbc-f1b9-f8a2c5dea395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing empirical analysis for TODO 1...\n",
            "Reflexivity Error ($\\epsilon_R$) vs $\\lambda_T$\n",
            "[(0.0, 0.9600136876106262), (0.1, 0.5553794503211975), (0.5, 0.4759384095668793), (1.0, 0.46787261962890625), (5.0, 0.4617440104484558), (10.0, 0.4609870910644531)]\n",
            "Structure MSE vs $\\lambda_T$\n",
            "[(0.0, 0.022634398192167282), (0.1, 0.042240120470523834), (0.5, 0.04993817210197449), (1.0, 0.05078938975930214), (5.0, 0.05148319900035858), (10.0, 0.05154617130756378)]\n",
            "lambda_T=0$ (Ring Focus)\n",
            "[[0.04014131 0.5397581  0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.03934409 0.03931469 0.5395105  0.04001489 0.03948966 0.03952682\n",
            "  0.03995282 0.03976201 0.040154   0.03930203]\n",
            " [0.04014131 0.04014131 0.04014131 0.5397581  0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.5397581  0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.5397581\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.5397581  0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.5397581  0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.5397581  0.04014131]\n",
            " [0.03949221 0.03945654 0.03986236 0.04004398 0.03962612 0.03965738\n",
            "  0.03999596 0.03984688 0.03941908 0.53907824]\n",
            " [0.5397581  0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]]\n",
            "lambda_T=10$ (Reflexive Focus)\n",
            "[[0.5390154  0.53970915 0.0397901  0.03980258 0.0400748  0.03998562\n",
            "  0.04004764 0.03999158 0.03978125 0.03987356]\n",
            " [0.03979252 0.5390136  0.53844523 0.03930143 0.03985902 0.03951388\n",
            "  0.03974861 0.03953513 0.04019892 0.03921413]\n",
            " [0.0396825  0.03970923 0.5389992  0.53808475 0.03976919 0.03932958\n",
            "  0.03962556 0.03935556 0.0408607  0.03905465]\n",
            " [0.04014131 0.04014131 0.04014131 0.5390159  0.5397581  0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.5390159  0.5397581\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.5390159\n",
            "  0.5397581  0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.5390159  0.5397581  0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.5390159  0.5397581  0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.5390159  0.5397581 ]\n",
            " [0.53948474 0.03976627 0.04033277 0.04015114 0.03981843 0.03943048\n",
            "  0.03969306 0.0394539  0.04048294 0.5390059 ]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# --- 1. CORE MLNN OPERATORS (Differentiable Kripke Semantics) ---\n",
        "\n",
        "def softmin(x, tau=0.1):\n",
        "    \"\"\"Equation 1 & Section 3.2.1: Sound lower bound on necessity quantification.\"\"\"\n",
        "    return -tau * torch.log(torch.sum(torch.exp(-x / tau), dim=-1) + 1e-8)\n",
        "\n",
        "def mlnn_box(A, L_phi):\n",
        "    \"\"\"\n",
        "    Implements Necessity Neuron (Section 3.2.1).\n",
        "    L_box = softmin( (1 - A_ij) + L_phi_j )\n",
        "    Calculates if phi is necessarily true in all worlds accessible from w.\n",
        "    \"\"\"\n",
        "    # A: [W, W] accessibility matrix\n",
        "    # L_phi: [W] truth values of proposition phi in each world\n",
        "    # Logic: If a world is accessible (A~1), L_phi must be high or the softmin collapses.\n",
        "    expanded_L = L_phi.unsqueeze(0).expand(A.size(0), -1)\n",
        "    return softmin((1.0 - A) + expanded_L)\n",
        "\n",
        "# --- 2. THE MLNN ARCHITECTURE ---\n",
        "\n",
        "class MLNN_Reasoner(nn.Module):\n",
        "    def __init__(self, num_worlds):\n",
        "        super().__init__()\n",
        "        self.num_worlds = num_worlds\n",
        "\n",
        "        # Accessibility Relation (A_theta): Using direct logits for stability in small worlds.\n",
        "        # Initialized with a \"Distrust Prior\" (Section 3.3, 7.3)\n",
        "        self.A_logits = nn.Parameter(torch.ones(num_worlds, num_worlds) * -2.0)\n",
        "\n",
        "        # Proposition Truth Bounds [L, U]: Section 3.1 & 3.2\n",
        "        # Initialize L < U to avoid immediate contradiction.\n",
        "        self.L_p = nn.Parameter(torch.rand(num_worlds) * 0.3)\n",
        "        self.U_p = nn.Parameter(0.7 + torch.rand(num_worlds) * 0.3)\n",
        "\n",
        "    def get_A(self):\n",
        "        \"\"\"Pass logits through sigmoid to ensure weights in [0, 1] (Section 142, 182).\"\"\"\n",
        "        return torch.sigmoid(self.A_logits)\n",
        "\n",
        "    def forward(self):\n",
        "        A = self.get_A()\n",
        "\n",
        "        # Evaluation: Necessity of p (Box p)\n",
        "        L_box_p = mlnn_box(A, self.L_p)\n",
        "\n",
        "        # Contradiction Loss (L_contra): Section 3.4\n",
        "        # Rule: Box p -> p. Contradiction occurs if L_box_p (premise) > U_p (conclusion).\n",
        "        l_contra = torch.mean(torch.relu(L_box_p - self.U_p)**2)\n",
        "\n",
        "        return A, l_contra\n",
        "\n",
        "# --- 3. THE EXPERIMENT LOOP (TODO 1) ---\n",
        "\n",
        "def run_reflexivity_sweep(lambda_t, num_worlds=10):\n",
        "    model = MLNN_Reasoner(num_worlds)\n",
        "    # Smaller learning rate to prevent structural collapse (Section 6.1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Ground Truth: Directed Ring (Section 5.6)\n",
        "    # i can see (i+1). This structure is NOT reflexive.\n",
        "    ring = torch.zeros(num_worlds, num_worlds)\n",
        "    for i in range(num_worlds):\n",
        "        ring[i, (i + 1) % num_worlds] = 1.0\n",
        "\n",
        "    for epoch in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        A, l_contra = model()\n",
        "\n",
        "        # Loss 1: Task Accuracy (Fitting the Ring)\n",
        "        l_task = torch.mean((A - ring)**2)\n",
        "\n",
        "        # Loss 2: Axiomatic Regularization (Equation 4 - Reflexivity)\n",
        "        # Forces the diagonal A[i,i] toward 1.0.\n",
        "        l_reflexive = torch.mean((1.0 - torch.diag(A))**2)\n",
        "\n",
        "        # Total Loss (Equation 3 + 4)\n",
        "        total_loss = l_task + (1.0 * l_contra) + (lambda_t * l_reflexive)\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Maintain valid bounds [0, 1] (Section 197)\n",
        "        with torch.no_grad():\n",
        "            model.L_p.clamp_(0, 1)\n",
        "            model.U_p.clamp_(0, 1)\n",
        "\n",
        "    # Calculate metrics for final state\n",
        "    final_A = model.get_A().detach()\n",
        "    epsilon_r = torch.mean(1.0 - torch.diag(final_A)).item()\n",
        "    task_mse = torch.mean((final_A - ring)**2).item()\n",
        "\n",
        "    return final_A.numpy(), epsilon_r, task_mse\n",
        "\n",
        "# --- 4. EXECUTION & VISUALIZATION ---\n",
        "lambdas = [0.0, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "plot_data = {'l': [], 'eps': [], 'mse': []}\n",
        "final_matrices = []\n",
        "\n",
        "print(\"Executing empirical analysis for TODO 1...\")\n",
        "for lt in lambdas:\n",
        "    matrix, eps, mse = run_reflexivity_sweep(lt)\n",
        "    plot_data['l'].append(lt)\n",
        "    plot_data['eps'].append(eps)\n",
        "    plot_data['mse'].append(mse)\n",
        "    final_matrices.append(matrix)\n",
        "\n",
        "# --- SAVE PLOTS TO INDIVIDUAL PDFS ---\n",
        "\n",
        "# 1. Reflexivity Error Plot (A)\n",
        "plt.figure(figsize=(6, 2))\n",
        "plt.plot(lambdas, plot_data['eps'], 'b-o', markersize=4)\n",
        "plt.title(\"Plot (a): Reflexivity Error ($\\\\epsilon_R$) vs $\\\\lambda_T$\")\n",
        "plt.xlabel(\"$\\\\lambda_T$\")\n",
        "plt.ylabel(\"$\\\\epsilon_R$\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('MLNN_AxiomT_Reflexivity_Error_Analysis.pdf')\n",
        "plt.close()\n",
        "\n",
        "print(\"Reflexivity Error ($\\\\epsilon_R$) vs $\\\\lambda_T$\")\n",
        "print(list(zip(lambdas, plot_data['eps'])))\n",
        "\n",
        "# 2. Task Performance Plot (B)\n",
        "plt.figure(figsize=(6, 2))\n",
        "plt.plot(lambdas, plot_data['mse'], 'r-s', markersize=4)\n",
        "plt.title(\"Plot (b): Structure MSE vs $\\\\lambda_T$\")\n",
        "plt.xlabel(\"$\\\\lambda_T$\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('MLNN_AxiomT_Task_Performance_Analysis.pdf')\n",
        "plt.close()\n",
        "\n",
        "print(\"Structure MSE vs $\\\\lambda_T$\")\n",
        "print(list(zip(lambdas, plot_data['mse'])))\n",
        "\n",
        "# 3. Heatmap Comparison Plot (C)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(6, 2))\n",
        "axes[0].imshow(final_matrices[0], cmap='viridis', vmin=0, vmax=1)\n",
        "axes[0].set_title(\"$\\\\lambda_T=0$ (Ring Focus)\")\n",
        "axes[1].imshow(final_matrices[-1], cmap='viridis', vmin=0, vmax=1)\n",
        "axes[1].set_title(\"$\\\\lambda_T=10$ (Reflexive Focus)\")\n",
        "for ax in axes: ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('MLNN_AxiomT_Structural_Heatmap_Comparison.pdf')\n",
        "plt.close()\n",
        "\n",
        "print(\"lambda_T=0$ (Ring Focus)\")\n",
        "print(final_matrices[0])\n",
        "print(\"lambda_T=10$ (Reflexive Focus)\")\n",
        "print(final_matrices[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Axiom $4$"
      ],
      "metadata": {
        "id": "IghY-bSjGd1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. CORE MLNN OPERATORS (Differentiable Kripke Semantics) ---\n",
        "\n",
        "def softmin(x, tau=0.1):\n",
        "    \"\"\"Equation 1 & Section 3.2.1: Sound lower bound on necessity.\"\"\"\n",
        "    return -tau * torch.log(torch.sum(torch.exp(-x / tau), dim=-1) + 1e-8)\n",
        "\n",
        "def mlnn_box(A, L_phi):\n",
        "    \"\"\"Implements Necessity Neuron (Section 3.2.1).\"\"\"\n",
        "    expanded_L = L_phi.unsqueeze(0).expand(A.size(0), -1)\n",
        "    return softmin((1.0 - A) + expanded_L)\n",
        "\n",
        "# --- 2. THE MLNN ARCHITECTURE ---\n",
        "\n",
        "class MLNN_Reasoner(nn.Module):\n",
        "    def __init__(self, num_worlds):\n",
        "        super().__init__()\n",
        "        self.num_worlds = num_worlds\n",
        "        self.A_logits = nn.Parameter(torch.ones(num_worlds, num_worlds) * -2.0)\n",
        "        self.L_p = nn.Parameter(torch.rand(num_worlds) * 0.3)\n",
        "        self.U_p = nn.Parameter(0.7 + torch.rand(num_worlds) * 0.3)\n",
        "\n",
        "    def get_A(self):\n",
        "        return torch.sigmoid(self.A_logits)\n",
        "\n",
        "    def forward(self):\n",
        "        A = self.get_A()\n",
        "        L_box_p = mlnn_box(A, self.L_p)\n",
        "        # Contradiction Loss (L_contra): Section 3.4\n",
        "        l_contra = torch.mean(torch.relu(L_box_p - self.U_p)**2)\n",
        "        return A, l_contra\n",
        "\n",
        "# --- 3. THE EXPERIMENT LOOP (TODO 1 - Adjusted for Axiom 4) ---\n",
        "\n",
        "def run_transitivity_sweep(lambda_4, num_worlds=10):\n",
        "    model = MLNN_Reasoner(num_worlds)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Ground Truth: Directed Ring (Section 5.6)\n",
        "    ring = torch.zeros(num_worlds, num_worlds)\n",
        "    for i in range(num_worlds):\n",
        "        ring[i, (i + 1) % num_worlds] = 1.0\n",
        "\n",
        "    for epoch in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        A, l_contra = model()\n",
        "\n",
        "        # Loss 1: Task Accuracy (Fitting the Ring)\n",
        "        l_task = torch.mean((A - ring)**2)\n",
        "\n",
        "        # Loss 2: Axiomatic Regularization (Equation 5 - Transitivity)\n",
        "        # Enforces: A_direct >= A_path_length_2 [cite: 323, 325]\n",
        "        A_sq = torch.matmul(A, A)\n",
        "        l_transitive = torch.mean(torch.relu(A_sq - A)**2)\n",
        "\n",
        "        # Total Loss (Equation 3 + 5)\n",
        "        total_loss = l_task + (1.0 * l_contra) + (lambda_4 * l_transitive)\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.L_p.clamp_(0, 1)\n",
        "            model.U_p.clamp_(0, 1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    final_A = model.get_A().detach()\n",
        "    # Transitivity Error: Magnitude of path violations\n",
        "    A_sq_final = torch.matmul(final_A, final_A)\n",
        "    epsilon_4 = torch.mean(torch.relu(A_sq_final - final_A)).item()\n",
        "    task_mse = torch.mean((final_A - ring)**2).item()\n",
        "\n",
        "    return final_A.numpy(), epsilon_4, task_mse\n",
        "\n",
        "# --- 4. EXECUTION & VISUALIZATION ---\n",
        "lambdas = [0.0, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "plot_data = {'l': [], 'eps': [], 'mse': []}\n",
        "final_matrices = []\n",
        "\n",
        "print(\"Executing empirical analysis for TODO 1 (Axiom 4)...\")\n",
        "for l4 in lambdas:\n",
        "    matrix, eps, mse = run_transitivity_sweep(l4)\n",
        "    plot_data['l'].append(l4)\n",
        "    plot_data['eps'].append(eps)\n",
        "    plot_data['mse'].append(mse)\n",
        "    final_matrices.append(matrix)\n",
        "\n",
        "# 1. Transitivity Error Plot\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(lambdas, plot_data['eps'], 'g-o', markersize=4)\n",
        "plt.title(\"Plot (a): Transitivity Error ($\\\\epsilon_4$) vs $\\\\lambda_4$\")\n",
        "plt.xlabel(\"$\\\\lambda_4$\")\n",
        "plt.ylabel(\"$\\\\epsilon_4$\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('MLNN_Axiom4_Transitivity_Error_Analysis.pdf')\n",
        "plt.close()\n",
        "\n",
        "print('Transitivity Error ($\\\\epsilon_4$) vs $\\\\lambda_4$')\n",
        "print(list(zip(lambdas, plot_data['eps'])))\n",
        "\n",
        "# 2. Task Performance Plot\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(lambdas, plot_data['mse'], 'r-s', markersize=4)\n",
        "plt.title(\"Plot (b): Structure MSE vs $\\\\lambda_4$\")\n",
        "plt.xlabel(\"$\\\\lambda_4$\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('MLNN_Axiom4_Transitivity_Task_Performance.pdf')\n",
        "plt.close()\n",
        "\n",
        "print('Structure MSE vs $\\\\lambda_4$')\n",
        "print(list(zip(lambdas, plot_data['mse'])))\n",
        "\n",
        "# 3. Heatmap Comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "axes[0].imshow(final_matrices[0], cmap='viridis', vmin=0, vmax=1)\n",
        "axes[0].set_title(\"$\\\\lambda_4=0$ (Ring Graph)\")\n",
        "axes[1].imshow(final_matrices[-1], cmap='viridis', vmin=0, vmax=1)\n",
        "axes[1].set_title(\"$\\\\lambda_4=10$ (Transitive Closure)\")\n",
        "for ax in axes: ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('MLNN_Axiom4_Transitivity_Heatmap_Comparison.pdf')\n",
        "plt.close()\n",
        "\n",
        "print(\"lambda_4=0$ (Ring Graph)\")\n",
        "print(final_matrices[0])\n",
        "print(\"lambda_4=10$ (Transitive Closure)\")\n",
        "print(final_matrices[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooatm4AsGfkQ",
        "outputId": "dfa4de07-9208-45a3-bd55-038b2a96f63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing empirical analysis for TODO 1 (Axiom 4)...\n",
            "Transitivity Error ($\\epsilon_4$) vs $\\lambda_4$\n",
            "[(0.0, 0.03934779390692711), (0.1, 0.03895629569888115), (0.5, 0.03733643889427185), (1.0, 0.03614405542612076), (5.0, 0.02606678567826748), (10.0, 0.018386483192443848)]\n",
            "Structure MSE vs $\\lambda_4$\n",
            "[(0.0, 0.022637639194726944), (0.1, 0.02294459566473961), (0.5, 0.024233411997556686), (1.0, 0.025784723460674286), (5.0, 0.034503377974033356), (10.0, 0.04063522443175316)]\n",
            "lambda_4=0$ (Ring Graph)\n",
            "[[0.04014131 0.5397581  0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.03958998 0.03975794 0.5395108  0.0400685  0.03999669 0.03997527\n",
            "  0.03959892 0.04003603 0.0399506  0.03957345]\n",
            " [0.0391896  0.03937645 0.03944352 0.539659   0.03983946 0.03979544\n",
            "  0.03918738 0.03992074 0.03974523 0.04106038]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.5397581  0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.5397581\n",
            "  0.04014131 0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.5397581  0.04014131 0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.5397581  0.04014131 0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.5397581  0.04014131]\n",
            " [0.04014131 0.04014131 0.04014131 0.04014131 0.04014131 0.04014131\n",
            "  0.04014131 0.04014131 0.04014131 0.5397581 ]\n",
            " [0.5387438  0.03940448 0.03946908 0.03999381 0.03985031 0.03980792\n",
            "  0.03922034 0.03992862 0.03975949 0.0410139 ]]\n",
            "lambda_4=10$ (Transitive Closure)\n",
            "[[0.03619516 0.3823489  0.10930141 0.05708776 0.0412183  0.03763231\n",
            "  0.0366945  0.03622587 0.03648081 0.03650168]\n",
            " [0.0366799  0.0368841  0.38274553 0.10938025 0.05726259 0.04163297\n",
            "  0.03779419 0.03680337 0.03664483 0.03664446]\n",
            " [0.03664426 0.03673149 0.0369141  0.38239253 0.10931908 0.05735851\n",
            "  0.04161824 0.03775972 0.0368492  0.03662845]\n",
            " [0.03664173 0.03665571 0.03676585 0.03689582 0.38258606 0.10948768\n",
            "  0.05738249 0.04160089 0.03779661 0.03684816]\n",
            " [0.03686098 0.03663632 0.03668652 0.03674451 0.03689445 0.38267553\n",
            "  0.109452   0.05732634 0.04162526 0.03779101]\n",
            " [0.03779981 0.03683976 0.03665339 0.03665074 0.03672746 0.03690141\n",
            "  0.3825462  0.10936014 0.05734749 0.04161144]\n",
            " [0.04162992 0.03777956 0.03686468 0.03662995 0.03664416 0.03674272\n",
            "  0.03689822 0.38251022 0.10942601 0.05734447]\n",
            " [0.05740165 0.0416188  0.03782193 0.03685078 0.03662867 0.0366668\n",
            "  0.03674949 0.03688547 0.382643   0.10943185]\n",
            " [0.10948715 0.05735029 0.04165103 0.03777898 0.03682214 0.03663754\n",
            "  0.03666168 0.03671506 0.03689413 0.38256016]\n",
            " [0.38286844 0.10946738 0.05742819 0.04159972 0.03773433 0.03684586\n",
            "  0.03664321 0.03660844 0.03673223 0.03686757]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Axiom $B$"
      ],
      "metadata": {
        "id": "xpDtQW_oH1ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. CORE MLNN OPERATORS ---\n",
        "\n",
        "def softmin(x, tau=0.1):\n",
        "    return -tau * torch.log(torch.sum(torch.exp(-x / tau), dim=-1) + 1e-8)\n",
        "\n",
        "def mlnn_box(A, L_phi):\n",
        "    expanded_L = L_phi.unsqueeze(0).expand(A.size(0), -1)\n",
        "    return softmin((1.0 - A) + expanded_L)\n",
        "\n",
        "# --- 2. THE MLNN ARCHITECTURE ---\n",
        "\n",
        "class MLNN_Reasoner(nn.Module):\n",
        "    def __init__(self, num_worlds):\n",
        "        super().__init__()\n",
        "        self.num_worlds = num_worlds\n",
        "        self.A_logits = nn.Parameter(torch.ones(num_worlds, num_worlds) * -2.0)\n",
        "        self.L_p = nn.Parameter(torch.rand(num_worlds) * 0.3)\n",
        "        self.U_p = nn.Parameter(0.7 + torch.rand(num_worlds) * 0.3)\n",
        "\n",
        "    def get_A(self):\n",
        "        return torch.sigmoid(self.A_logits)\n",
        "\n",
        "    def forward(self):\n",
        "        A = self.get_A()\n",
        "        L_box_p = mlnn_box(A, self.L_p)\n",
        "        l_contra = torch.mean(torch.relu(L_box_p - self.U_p)**2)\n",
        "        return A, l_contra\n",
        "\n",
        "# --- 3. THE EXPERIMENT LOOP (Axiom B) ---\n",
        "\n",
        "def run_symmetry_sweep(lambda_b, num_worlds=10):\n",
        "    model = MLNN_Reasoner(num_worlds)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Ground Truth: Directed Ring\n",
        "    ring = torch.zeros(num_worlds, num_worlds)\n",
        "    for i in range(num_worlds):\n",
        "        ring[i, (i + 1) % num_worlds] = 1.0\n",
        "\n",
        "    for epoch in range(250):\n",
        "        optimizer.zero_grad()\n",
        "        A, l_contra = model()\n",
        "\n",
        "        # Loss 1: Task Accuracy\n",
        "        l_task = torch.mean((A - ring)**2)\n",
        "\n",
        "        # Loss 2: Axiomatic Regularization (Equation 6 - Symmetry)\n",
        "        # Penalizes A[i,j] != A[j,i]\n",
        "        l_symmetric = torch.mean((A - A.transpose(0, 1))**2)\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss = l_task + (1.0 * l_contra) + (lambda_b * l_symmetric)\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.L_p.clamp_(0, 1)\n",
        "            model.U_p.clamp_(0, 1)\n",
        "\n",
        "    final_A = model.get_A().detach()\n",
        "    # Symmetry Error: Difference between A and its transpose\n",
        "    epsilon_b = torch.mean(torch.abs(final_A - final_A.transpose(0, 1))).item()\n",
        "    task_mse = torch.mean((final_A - ring)**2).item()\n",
        "\n",
        "    return final_A.numpy(), epsilon_b, task_mse\n",
        "\n",
        "# --- 4. EXECUTION ---\n",
        "lambdas = [0.0, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "plot_data = {'l': [], 'eps': [], 'mse': []}\n",
        "final_matrices = []\n",
        "\n",
        "print(\"Executing Symmetry Analysis (Axiom B)...\")\n",
        "for lb in lambdas:\n",
        "    matrix, eps, mse = run_symmetry_sweep(lb)\n",
        "    plot_data['l'].append(lb)\n",
        "    plot_data['eps'].append(eps)\n",
        "    plot_data['mse'].append(mse)\n",
        "    final_matrices.append(matrix)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "axes[0].imshow(final_matrices[0], cmap='magma', vmin=0, vmax=1)\n",
        "axes[0].set_title(\"$\\\\lambda_B=0$ (Directed Ring)\")\n",
        "axes[1].imshow(final_matrices[-1], cmap='magma', vmin=0, vmax=1)\n",
        "axes[1].set_title(\"$\\\\lambda_B=10$ (Symmetric Graph)\")\n",
        "for ax in axes: ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('MLNN_AxiomB_Symmetry_Heatmap_Comparison.pdf')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "print(\"lambda_B=0$ (Directed Ring)\")\n",
        "print(final_matrices[0])\n",
        "print(\"lambda_B=10$ (Symmetric Graph)\")\n",
        "print(final_matrices[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhKVotqZIJYy",
        "outputId": "2de399ed-8742-4ff1-da9a-a4697de49742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Symmetry Analysis (Axiom B)...\n",
            "lambda_B=0$ (Directed Ring)\n",
            "[[0.0347987  0.6378795  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.0347987  0.0347987 ]\n",
            " [0.0347987  0.0347987  0.6378795  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.0347987  0.0347987 ]\n",
            " [0.0347987  0.0347987  0.0347987  0.6378795  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.0347987  0.0347987 ]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.6378795  0.0347987\n",
            "  0.0347987  0.0347987  0.0347987  0.0347987 ]\n",
            " [0.03475046 0.03471795 0.03442907 0.03452768 0.03467333 0.63785034\n",
            "  0.03438036 0.03451167 0.03470337 0.03457715]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.6378795  0.0347987  0.0347987  0.0347987 ]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.6378795  0.0347987  0.0347987 ]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.6378795  0.0347987 ]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.0347987  0.6378795 ]\n",
            " [0.6378795  0.0347987  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.0347987  0.0347987 ]]\n",
            "lambda_B=10$ (Symmetric Graph)\n",
            "[[0.0347987  0.48779535 0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.03478071 0.4640577 ]\n",
            " [0.4641652  0.0347987  0.48779535 0.0347987  0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.03477316 0.03462901]\n",
            " [0.0347987  0.4641652  0.0347987  0.48779535 0.0347987  0.0347987\n",
            "  0.0347987  0.0347987  0.03478171 0.03468091]\n",
            " [0.0347987  0.0347987  0.4641652  0.0347987  0.48779535 0.0347987\n",
            "  0.0347987  0.0347987  0.03474496 0.03453585]\n",
            " [0.0347987  0.0347987  0.0347987  0.4641652  0.0347987  0.48779535\n",
            "  0.0347987  0.0347987  0.03476464 0.03458388]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.4641652  0.0347987\n",
            "  0.48779535 0.0347987  0.03478483 0.03470131]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.0347987  0.4641652\n",
            "  0.0347987  0.48779535 0.03479356 0.03476145]\n",
            " [0.0347987  0.0347987  0.0347987  0.0347987  0.0347987  0.0347987\n",
            "  0.4641652  0.0347987  0.4878677  0.03453564]\n",
            " [0.03477329 0.03476281 0.03477471 0.03472459 0.03475102 0.03477908\n",
            "  0.0347914  0.4642493  0.03478536 0.4878391 ]\n",
            " [0.4876806  0.03457029 0.03463813 0.034501   0.03451573 0.03466555\n",
            "  0.03474746 0.03447599 0.4642191  0.03464644]]\n"
          ]
        }
      ]
    }
  ]
}